export const meta = {
  slug: "gridworld",
  title: "Interactive Gridworld",
  creation: "2019-01-15",
  updated: "2022-03-12",
  summary: `An easy introduction to Markov Decision Processes and Reinforcement Learning by the mean of a gridworld environement
    where you can play with different parameters of solving algorithms and modify the map to see the effects.
    We'll see what are state-value function and policies and we look into four solvers.`,
  categories: ["Artificial Intelligence"],
  tags: ["Reinforcement Learning", "Markov Decision Process", "Gridworld"],
};

import Gridworld from "@/components/Gridworld";

# Interactive Gridworld

<Gridworld />

## What is a gridworld ?

This gridworld is 8 lines x 12 columns.
It represent an _environment_ in which our _agent_ is going to take _actions_.
The orange cell placed on the bottom right is the goal _state_, where we want to go.
You can move the ending point by draging it.
Walls have been placed and are represented by blue cells.
You can place other ones by clicking or initiating a drag on an empty cell.
Similarly, you can remove walls by clicking or initiating a drag on a wall cell.

![Finite Deterministic Markov Decision Process](/static/posts/gridworld/markov_decision_process.jpg)

We start at the first time step on the starting point $s_0$, with an initial reward $r_0$ being $0$.
At each time step $t$, we chose a possible action $a_t$.
Let's say that we chosed to go down $a_0 = down$.
If there is nothing or a wall, we get a 'reward' of $r_1 = -1$. And if we got to the end, we get a reward of $r_1 = +100$ and the game ends.
Note that we can define a terminal _state_ as a _state_ where every action takes you to the same _state_ with a reward of $0$.
The goal is to finish the game with the maximum (discounted) reward possible $\sum_{t=0}^{+\infty} \gamma^t r_{t+1}$.
That is why we give a negative reward when transitionning to a cell where there is nothing.
This tells us that the shortest path to the end is going to be better than making 3 times the tour of the map before going to the goal _state_.

The theory lying underneath that will help us understand the way we design solving algorithms for this kind of problems is **Reinforcement Learning**.  
It is based on a mathematical model that capture the essence of problem like this one called [**Markov Decision Process**][1].  
In a deterministic **MDP**, you have:

- a set of _states_ $S$, where $S_t$ is the subset of _states_ you can access at time step $t$.
- a set of _actions_ $A$, where $A_t$ is the subset of _actions_ you can chose at time step $t$.
- a _reward_ function $R: S \times S \rightarrow \mathbb{R}$, where $R(s, s')$ is the _reward_ you get when transitionning from _state_ $s$ to $s'$.

Note that I sometimes use the $R$ function as a function defined like this instead $R: S \times A \rightarrow \mathbb{R}$ 
where $R(s,a)$ is the reward you get from transitioning from $s$ to the state you end up in by following $a$.

## What is a policy, a state value and a Q-function ?

We first define the concept of _policy_.
The policy is a function $\pi : S \rightarrow A$. In other words,
it takes a _state_ as an input and output an _action_ to take.
An agent is said to follow a policy $\pi$ if $\forall t \in \mathbb{N}, a_t = \pi(s_t)$ i.e. if it takes the action that the policy asks it to follow.  
We can now define what are a state-value and a Q-value (also called action-value) function given a policy:

$$
\begin{aligned}
  V^{\pi}(s_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1} \mid s_t \right] \\
  Q^{\pi}(s_t, a_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1} \mid s_t, a_t \right]
\end{aligned}
$$

We already used $0 < \gamma \leq 1$ three times without explaining what it was, it is called the _discount rate_.
It will set at what extent our agent is going to be concerned by long-time reward.
If $\gamma$ is near $0$, we are going to take into account only reward in the near future.
If, however, $\gamma$ is near $1$, we will considerate more long-lasting reward.  
Ok now, what about $V^{\pi}$ and $Q^{\pi}$ ?  
$V^{\pi}(s_t)$ represents the future discounted reward, starting at $s_t$ and following the policy $\pi$.  
$Q^{\pi}(s_t, a_t)$ is very similar, it is the future discounted reward, still starting at $s_t$
but this time taking action $a_t$ (possilbly different from $\pi(s_t)$) before following the policy.

A policy which is going to maximize the future dicounted reward is called an optimal policy.
There can be several ones sometimes (for instance if transitioning to nothing would yield a reward of 0 and if the discounting factor was 1) or if two paths from start to end have the same lengths.
Let's take one, we are going to call it $\pi^*$, we also define the optimal
state-value function $V^* = V^{\pi^*} = \max_{\pi} V^{\pi}$
as well as the optimal Q-value function $Q^* = Q^{\pi^*} = \max_{\pi} Q^{\pi}$. Note that all optimal policies share the same state-value and action-value (Q-value) function because if one was higher than the others, the other ones wouldn't be optimal.

## Let's solve it !

The next remarks are going to be central for solving the problem,
they are the [Bellman Equations][2] of a deterministic _MDP_ (with deterministic policy).\
Note that here $s: S \times A \rightarrow S$ is a function that takes a state and action and returns a state since we are in the deterministic case.\

$$
\begin{aligned}
  V^{\pi}(s_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1}\\
  &= r_{t+1} + \sum_{i = t+1}^{+ \infty} \gamma^{i-t} r_{i+1}\\
  &= r_{t+1} + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_{i+1}\\
  &= R(s_t, s(s_t, \pi(s_t))) + \gamma V^{\pi}(s(s_t, \pi(s_t)))
\end{aligned}
$$

By the same reasoning we obtain the equation for the action-value function:

$$
\begin{aligned}
  Q^{\pi}(s_t, a_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1} \mid a_t\\
  &= r_{t+1} + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_{i+1}\\
  &= R(s_t, s(s_t, a_t)) + \gamma V^{\pi}(s(s_t, a_t))
\end{aligned}
$$

For the optimal state-value and action-value functions, we have:

$$
\begin{aligned}
  V^*(s_t) &= R(s_t, s(s_t,\pi^*(s_t))) + \gamma V^*(s(s_t,\pi^*(s_t)))\\
  Q^*(s_t, a_t) &= R(s_t, s(s_t, a_t)) + \gamma V^*(s(s_t, a_t))
\end{aligned}
$$

We can also remark that $V^*(s) = \max_a Q^*(s, a)$ and inject it in the last equation:

$$
Q^*(s_t, a_t) = R(s_t, s(s_t, a_t)) + \gamma \max_{a_{t+1}} Q^*(s(s_t, a_t), a_{t+1})
$$

## Code preliminaries

Those code examples are going to be written in Rust. We start by defining the following types:

```rust
#[derive(Debug, PartialEq)]
pub enum Cell {
    Air,
    Wall,
    End,
}

#[derive(Eq, Hash, PartialEq, Debug, Clone, Copy)]
pub enum Policy {
    Up,
    Down,
    Left,
    Right,
}

pub type Grid<T> = Vec<Vec<T>>;
pub type ActionValue = HashMap<Policy, f64>;
```

I also wrote some helper functions to make the code more readable from which you can find all the functions [here][5].
Also, note that given the similarity between the implementations of Policy Iteration and Value Iteration and those of SARSA and Q-Learning,
I will have their implementations in 2 shared functions.

## Policy Iteration

The first method we are going to use is called Policy Iteration.
We initialize the state-value function with a random one or we can also initialize it at $0$ for every state.
We, then, derive a better policy from this state-value.
We calculate the new state-value and again a new better policy,
and so on until the policy is stable.

### Policy Evaluation

We first need a method in order to estimate the state-value function of a policy.
This technique is called _Policy Evaluation_.
For this, we want to use the Bellman equation as an update rule for our state-value estimation:

$$
V_{k+1}(s) = R(s, \pi(s)) + \gamma V_k(\pi(s))
$$

We can then have two arrays, one for the old values and one for the new values calculated from the old ones.
We can also do it in-place with one array, replacing values as we go through $S$.
Although the latter is usually faster to converge, it is anisotropic in the sense that
the order in which we are going to do the updates is going to matter.
In this example, we are going to use the two array version.

```rust
/// Evaluates the policy for a given state and optional initial state value grid
/// Returns a new state value grid
pub fn policy_evaluation(
    cell_grid: &Grid<Cell>,
    policy_grid: &Grid<Policy>,
    gamma: Option<f64>,
    iter_before_improvement: Option<u32>,
    state_value_grid: Option<&Grid<f64>>,
) -> Grid<f64> {
    let (n, m) = get_grid_size(&cell_grid);
    let mut state_value = match state_value_grid {
        Some(state_value) => state_value.clone(),
        None => matrix(n, m, 0.0),
    };
    let gamma = gamma.unwrap_or(DEFAULT_GAMMA);
    let early_stop = match iter_before_improvement {
        Some(_) => true,
        None => false,
    };
    let mut iteration = 0;
    loop {
        iteration += 1;
        let mut delta: f64 = 0.0;
        for i in 0..n {
            for j in 0..m {
                let (i_, j_, reward) = transition(&cell_grid, i, j, &policy_grid[i][j]);
                let new_state_value = reward as f64 + gamma * state_value[i_][j_];
                delta = delta.max((new_state_value - state_value[i][j]).abs());
                state_value[i][j] = new_state_value;
            }
        }
        if delta < 1e-5 || (early_stop && iteration == iter_before_improvement.unwrap()) {
            break;
        }
    }
    state_value
}
```

The `transition` function is a helper function that returns the next _state_ and the reward given a _state_ and an _action_. And the reason we have an early stop implemented in this function is that we are going to use it in the next solver: Value Iteration.

### Policy Improvement

Once we have evaluated the state-value function,
we change our policy for a better one according to this state-value function:

$$
\pi'(s) = \text{argmax}_a [r(s, s(s, a)) + \gamma V(s(s, a))]
$$

```rust
/// Improves the policy in place for a given state value grid
/// Returns a boolean to see if policy is stable
pub fn policy_improvement(
    policy: &mut Grid<Policy>,
    cell_grid: &Grid<Cell>,
    state_value_grid: &Grid<f64>,
    gamma: Option<f64>,
) -> bool {
    let (n, m) = get_grid_size(state_value_grid);
    let gamma = gamma.unwrap_or(DEFAULT_GAMMA);
    let mut is_stable = true;
    for i in 0..n {
        for j in 0..m {
            let mut max_reward = -1.0;
            let mut max_cell_policy = Policy::Up;
            for cell_policy in get_policy_directions() {
                let (i_, j_, r) = transition(&cell_grid, i, j, &cell_policy);
                let reward = r as f64 + gamma * state_value_grid[i_][j_];
                if reward > max_reward {
                    max_reward = reward;
                    max_cell_policy = cell_policy;
                }
            }
            if (*policy)[i][j] != max_cell_policy {
                is_stable = false;
            }
            (*policy)[i][j] = max_cell_policy;
        }
    }
    is_stable
}
```

### Final Algorithm

![Policy Iteration](/static/posts/gridworld/policy_iteration.svg)

## Value Iteration

In Policy Iteration, we were estimating the state-value function of the policies until reasonable convergence.
In Value Iteration, we instantly greedify the policy between the sweeps for state-value evaluation.
It discards the need for actually computing the policy between each evaluation as it just computes the greedy one on the fly at the moment of choosing an action.
In general, a variant of that algorithm is used where we do a few steps of evaluation between two policy improvement steps.
In this case, the code will be the said variant.

![Value Iteration](/static/posts/gridworld/value_iteration.svg)


```rust
pub fn policy_value_iteration(
    cell_grid: Grid<Cell>,
    gamma: Option<f64>,
    iter_before_improvement: Option<u32>,
) -> (Grid<f64>, Grid<Policy>) {
    let (n, m) = get_grid_size(&cell_grid);
    let mut is_stable = false;
    let mut state_value_grid = matrix(n, m, 0.0);
    let mut policy_grid = matrix(n, m, Policy::Up);
    while !is_stable {
        state_value_grid = policy_evaluation(
            &cell_grid,
            &policy_grid,
            gamma,
            iter_before_improvement,
            Some(&state_value_grid),
        );
        is_stable = policy_improvement(&mut policy_grid, &cell_grid, &state_value_grid, gamma);
    }
    (state_value_grid, policy_grid)
}
```

In this shared implementation we now see why we have the `iter_before_improvement` parameter.
It is used to do a few steps of evaluation before improving the policy in the case of Value Iteration.
If it is set to None, we evaluate until convergence so the Policy Iteration algorithm is used.

## SARSA

With SARSA and $Q$-Learning we are going to be estimating $Q$ instead of $V$ and derive the optimal policy from it.
Another difference with these methods is that we are going to be simulating episodes
through the gridworld in order to estimate the $Q$-function instead of sweeping through the _state_ space $S$.
SARSA stands for State-Action-Reward-State-Action because we are going to look one action forward following the policy to estimate $Q(s,a)$.
In fact we are not always going to follow the policy because it can be biased if it has not tried certain actions.
To ensure that we explore the state-action space we need to take a random action once in a while and to start at a random valid position on the grid.
Since we don't need to explore as much as in the beginning episodes after episodes we can decrease the probability over time.
This probability of chosing a random action in a given _state_ is represented by $\varepsilon$
which decreases in $\frac{1}{t}$ where $t$ represent the epsiode.

```rust
pub fn epsilon_greedy(
    action_value: &ActionValue,
    epsilon_0: f64,
    t: u32,
    exploration_period: u32,
) -> Policy {
    let mut rng = rand::thread_rng();
    let epsilon = epsilon_0 / (1.0 + (t / exploration_period + 1) as f64);
    if rng.gen::<f64>() < epsilon {
        choose_random_action()
    } else {
        greedy(action_value)
    }
}
```

Here we use $\varepsilon = \frac{\varepsilon_0}{1+t/T}$.
$\varepsilon_0$ is $\varepsilon$ at time-step $t=0$ and $T$ represents the decreasing period.
Writing it this way also prevents from dividing by $0$ if $t=0$.
I also added $1$ to the exploration period because we don't want to divide by $0$ in the first episode.
Choosing a random action with probability $\varepsilon$ and the greedy action otherwise
is called an $\varepsilon$-greedy policy.

## Q-Learning

Q-Learning is basically the same as SARSA, it only differs in its update rule:
instead of choosing a second action following the $\varepsilon$-greedy policy,
we choose the best _action_ available in _state_ $s'$: $\argmax_{a'} a' \rightarrow Q(s', a')$

```rust
pub fn sarsa_q_learning(
    cell_grid: Grid<Cell>,
    q_learning: bool,
    num_episodes: Option<u32>,
    alpha: Option<f64>,
    gamma: Option<f64>,
    epsilon_0: Option<f64>,
    exploration_period: Option<u32>,
) -> Grid<ActionValue> {
    // Default values
    let num_episodes = num_episodes.unwrap_or(DEFAULT_NUM_EPISODES);
    let alpha = alpha.unwrap_or(DEFAULT_ALPHA);
    let gamma = gamma.unwrap_or(DEFAULT_GAMMA);
    let epsilon_0 = epsilon_0.unwrap_or(DEFAULT_EPSILON_0);
    let exploration_period = exploration_period.unwrap_or(DEFAULT_EXPLORATION_PERIOD);

    let (n, m) = get_grid_size(&cell_grid);
    let mut action_value_grid = new_action_value_grid(n, m);

    for episode in 0..num_episodes {
        let mut step = 0;
        let (mut i, mut j) = choose_random_state(&cell_grid);
        let action_value = &action_value_grid[i][j];
        let mut action = epsilon_greedy(action_value, epsilon_0, episode, exploration_period);

        while &cell_grid[i][j] != &Cell::End && step < MAX_NUM_STEPS {
            step += 1;
            let (i_, j_, reward) = transition(&cell_grid, i, j, &action);
            let next_action_value = &action_value_grid[i_][j_];
            let next_action =
                epsilon_greedy(next_action_value, epsilon_0, episode, exploration_period);
            let q_value = if q_learning {
                max_action_value(&action_value_grid[i_][j_])
            } else {
                *action_value_grid[i_][j_].get(&next_action).unwrap()
            };
            if let Some(action_value) = action_value_grid[i][j].get_mut(&action) {
                *action_value += alpha * (reward as f64 + gamma * q_value - *action_value);
            }
            (i, j, action) = (i_, j_, next_action);
        }
    }

    action_value_grid
}
```

Remark that we do not completely update $Q(s,a)$ to the new computed value.
We make a step in that direction of proportion $\alpha$, this parameter is the
learning rate of our algorithm. It is used to make the algorithm more stable.

## Final Note

If you want to see the actual code used here (TypeScript React), look [here][3].
Also, a lot of simplifications have been done in order to explain more directly
the different concepts. Finally, we only looked at the case of deterministic MDP with known model.
To have a better view of **Reinforcement Learning**, I recommend this [excellent book][4] about it
written by Sutton & Barto, the visual representations from this article are actually taken from that book.

[1]: https://en.wikipedia.org/wiki/Markov_decision_process
[2]: https://en.wikipedia.org/wiki/Bellman_equation
[3]: https://github.com/devspaceship/devspaceship.com/tree/main/web/src/components/Gridworld
[4]: http://incompleteideas.net/book/bookdraft2017nov5.pdf
[5]: https://github.com/devspaceship/devspaceship.com/blob/main/gridworld/src/utils.rs