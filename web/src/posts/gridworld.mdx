export const meta = {
  slug: "gridworld",
  title: "Interactive Gridworld",
  creation: "2019-01-15",
  updated: "2024-02-14",
  summary: `An easy introduction to Markov Decision Processes and Reinforcement Learning by the mean of a gridworld environment
    where you can play with different parameters of solving algorithms and modify the map to see the effects.
    We'll see what are state-value function and policies and we look into four solvers.`,
  categories: ["Artificial Intelligence"],
  tags: ["Reinforcement Learning", "Markov Decision Process", "Gridworld"],
};

import Gridworld from "components/Gridworld";
import CodeAccordion from "ui/custom/CodeAccordion";

# Interactive Gridworld

<Gridworld />

## What is a gridworld ?

A _gridworld_ represents an _environment_ in which an _agent_ is able to take _actions_.
The orange cell placed on the bottom right is the goal _state_, where we want to go.
You can move the ending point by draging it.
Walls have been placed and are represented by blue cells.
You can place other ones by clicking or initiating a drag on an empty cell.
Similarly, you can remove walls by clicking or initiating a drag on a wall cell.

![Finite Deterministic Markov Decision Process](/static/posts/gridworld/markov_decision_process.png)

We start at the first time step on the starting point $s_0$, with an initial reward $r_0$ being $0$.
At each time step $t$, we chose a possible _action_ $a_t$.
Let's say that we chosed to go down $a_0 = down$.
If there is nothing or a wall, we get a 'reward' of $r_1 = -1$. And if we got to the end, we get a reward of $r_1 = +100$ and the game ends.
Note that we can define a terminal _state_ as a _state_ where every _action_ takes you to the same _state_ with a reward of $0$.
The goal is to finish the game with the maximum (discounted) reward possible $\sum_{t=0}^{+\infty} \gamma^t r_{t+1}$.
That is why we give a negative reward when transitionning to a cell where there is nothing.
This tells us that the shortest path to the end is going to be better than making 3 times the tour of the map before going to the goal _state_.\
You might be wondering what this $\gamma$ thing is.
It is a parameter between $0$ and $1$ called the _discount factor_.
It indicates how much we care about rewards that are far in the future.
If $\gamma$ is near $0$, we are going to take into account only reward in the near future.
If, however, $\gamma$ is near $1$, we will considerate more long-lasting reward as well.

## Markov Decision Process

You might also be wondering why we are defining the problem like this.
That is because it is a very general way to define a problem where an _agent_ is going to take _actions_ in an _environment_.
This is central to the field of **Reinforcement Learning**.
The underlying mathematical model is called a [**Markov Decision Process**][1].
In this example, we will keep it simple and consider a deterministic _MDP_.
In a deterministic **MDP**, the _state_ you end up in after taking a certain _action_ in a certain _state_ is always the same.
Another simplification we are going to make is that the _reward_ you get when transitioning
is independent of the _action_ you took and only depends on the _state_ you are transitioning to and the one you are transitiong from.
Such an **MDP** is defined by:

- A set of _states_ $S$
- A set of _actions_ $A$
- A _transition_ function $s: S \times A \rightarrow S$ that takes a _state_ and an _action_ and returns the next _state_
- A _reward_ function $R: S \times S \rightarrow \mathbb{R}$ that takes a _state_ and the next _state_ and returns a _reward_

## What is a policy, a _state_ _value_ and a Q-function ?

We first define the concept of _policy_.
The policy is a function $\pi : S \rightarrow A$. In other words,
it takes a _state_ as an input and output an _action_ to take.
An agent is said to follow a policy $\pi$ if at every timestep $t$, $a_t = \pi(s_t)$ i.e. if it takes the _action_ that the policy asks it to follow.
We can now define what are a _state-value_ and a Q-_value_ (also called _action-value_) function given a policy:

$$
\begin{aligned}
  V^{\pi}(s_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1} \mid s_t \right] \\
  Q^{\pi}(s_t, a_t) &= \mathbb{E}_{\pi} \left[\sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1} \mid s_t, a_t \right]
\end{aligned}
$$

$V^{\pi}(s_t)$ represents the future discounted reward, starting at $s_t$ and following the policy $\pi$.
$Q^{\pi}(s_t, a_t)$ is very similar, it is the future discounted reward, still starting at $s_t$
but this time taking _action_ $a_t$ (possilbly different from $\pi(s_t)$) before following the policy.

A policy which is going to maximize the future dicounted reward is called an optimal policy.
There can be several ones sometimes (for instance if transitioning to nothing would yield a reward of 0 and if the discounting factor was 1) or if two paths from start to end have the same lengths.
Let's take one, we are going to call it $\pi^*$, we also define the optimal
_state-value_ function $V^* = V^{\pi^*} = \max_{\pi} V^{\pi}$
as well as the optimal Q-_value_ function $Q^* = Q^{\pi^*} = \max_{\pi} Q^{\pi}$.
Note that all optimal policies share the same _state-value_ and _action-value_ (Q-_value_) function because if a policy was better than an optimal one, it would not be optimal.

## Let's solve it !

The next remarks are going to be central for solving the problem,
they are the [Bellman Equations][2] of a deterministic _MDP_ (with deterministic policy).

$$
\begin{aligned}
  V^{\pi}(s_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1}\\
  &= r_{t+1} + \sum_{i = t+1}^{+ \infty} \gamma^{i-t} r_{i+1}\\
  &= r_{t+1} + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_{i+1}\\
  &= R(s_t, s(s_t, \pi(s_t))) + \gamma V^{\pi}(s(s_t, \pi(s_t)))
\end{aligned}
$$

By the same reasoning we obtain the equation for the _action-value_ function:

$$
\begin{aligned}
  Q^{\pi}(s_t, a_t) &= \sum_{i = t}^{+ \infty} \gamma^{i-t} r_{i+1} \mid a_t\\
  &= r_{t+1} + \gamma \sum_{i = t+1}^{+ \infty} \gamma^{i-(t+1)} r_{i+1}\\
  &= R(s_t, s(s_t, a_t)) + \gamma V^{\pi}(s(s_t, a_t))
\end{aligned}
$$

For the optimal _state-value_ and _action-value_ functions, we have:

$$
\begin{aligned}
  V^*(s_t) &= R(s_t, s(s_t,\pi^*(s_t))) + \gamma V^*(s(s_t,\pi^*(s_t)))\\
  Q^*(s_t, a_t) &= R(s_t, s(s_t, a_t)) + \gamma V^*(s(s_t, a_t))
\end{aligned}
$$

We can also remark that $V^*(s) = \max_a Q^*(s, a)$.
This is because the maximum discounted reward from a _state_ is the one that you would get by following the best _action_ available in that _state_.
We can now inject this in the equation:

$$
Q^*(s_t, a_t) = R(s_t, s(s_t, a_t)) + \gamma \max_{a_{t+1}} Q^*(s(s_t, a_t), a_{t+1})
$$

This is called the _Bellman Optimality Equation_.

## Code preliminaries

Those code examples are going to be written in Rust ðŸ¦€.
We will be using a crate that I wrote for this purpose called [`madepro`](https://docs.rs/madepro/0.1.0/madepro/) (for **Markov Decision Processes**).
The code examples will be from the _environment_ part of the library where the _gridworld_ is defined.
I will just show the part of the code that is relevant to the concepts we talked about.
Some details like implementations of certain functions will be ommited as they are not relevant to the concepts we are talking about.
We start by defining types to represent the _gridworld_ and the different concepts we talked about:

<CodeAccordion>

```rust
use madepro::models::{Action, Sampler, State, MDP};

const END_TRANSITION_REWARD: f64 = 100.0;
const NO_OP_TRANSITION_REWARD: f64 = -1.0;

/// A gridworld state (i, j)
#[derive(PartialEq, Eq, Hash, Debug, Clone)]
pub struct GridworldState {
    i: usize,
    j: usize,
}

impl State for GridworldState {}

/// A gridworld action
#[derive(PartialEq, Eq, Hash, Debug, Clone)]
pub enum GridworldAction {
    Down,
    Left,
    Right,
    Up,
}

impl Action for GridworldAction {}

/// A gridworld cell
#[derive(Debug, PartialEq)]
pub enum Cell {
    Air,
    Wall,
    End,
}

/// A gridworld
pub struct Gridworld {
    cell_grid: Vec<Vec<Cell>>,
    states: Sampler<GridworldState>,
    actions: Sampler<GridworldAction>,
}

impl Gridworld {
    /// Returns the grid's width and height
    fn get_grid_size(&self) -> (usize, usize) {
        (self.cell_grid.len(), self.cell_grid[0].len())
    }
}

impl MDP for Gridworld {
    type State = GridworldState;
    type Action = GridworldAction;

    fn get_states(&self) -> &Sampler<Self::State> {
        &self.states
    }

    fn get_actions(&self) -> &Sampler<Self::Action> {
        &self.actions
    }

    fn is_state_terminal(&self, state: &Self::State) -> bool {
        let cell = &self.cell_grid[state.i][state.j];
        *cell == Cell::End
    }

    fn transition(&self, state: &Self::State, action: &Self::Action) -> (Self::State, f64) {
        let cell = &self.cell_grid[state.i][state.j];

        // Edge cases
        // In theory the Cell::Wall case should never happen
        if (*cell) == Cell::End || (*cell) == Cell::Wall {
            return (state.clone(), 0.0);
        }

        // Tentative position
        let (i, j) = (state.i as i32, state.j as i32);
        let (i_, j_) = match action {
            Self::Action::Up => (i - 1, j),
            Self::Action::Down => (i + 1, j),
            Self::Action::Left => (i, j - 1),
            Self::Action::Right => (i, j + 1),
        };

        // Check out of bounds
        let (n, m) = self.get_grid_size();
        let (n, m) = (n as i32, m as i32);
        if i_ < 0 || i_ >= n || j_ < 0 || j_ >= m {
            return (state.clone(), NO_OP_TRANSITION_REWARD);
        }

        // Result
        let (i_, j_) = (i_ as usize, j_ as usize);
        let cell_ = &self.cell_grid[i_][j_];
        match cell_ {
            Cell::Air => (Self::State::new(i_, j_), NO_OP_TRANSITION_REWARD),
            Cell::Wall => (state.clone(), NO_OP_TRANSITION_REWARD),
            Cell::End => (Self::State::new(i_, j_), END_TRANSITION_REWARD),
        }
    }
}
```

</CodeAccordion>

## Policy Iteration

The first method we are going to use is called Policy Iteration.
We initialize the _state-value_ function with a random one or we can also initialize it at $0$ for every _state_.
We, then, derive a better policy from this _state-value_.
We calculate the new _state-value_ and again a new better policy,
and so on until the policy is stable.

### Policy Evaluation

We first need a method in order to estimate the _state-value_ function of a policy.
This technique is called _Policy Evaluation_.
For this, we want to use the Bellman equation as an update rule for our _state-value_ estimation:

$$
V_{k+1}(s) = R(s, \pi(s)) + \gamma V_k(\pi(s))
$$

We can then have two arrays, one for the old _values_ and one for the new _values_ calculated from the old ones.
We can also do it in-place with one array, replacing _values_ as we go through $S$.
Although the latter is usually faster to converge, it is anisotropic in the sense that
the order in which we are going to do the updates is going to matter.
This example is from the implementation of the library [madepro][6].

<CodeAccordion>

```rust
/// # Policy Evaluation
///
/// This function implements the policy evaluation algorithm.
/// It works by using the Bellman equation to iteratively update the state value.
/// The algorithm stops when the state value converge.
/// If the `iterations_before_improvement` parameter is set,
/// the algorithm will stop early after the given number of iterations.
pub fn policy_evaluation<M>(
    mdp: &M,
    config: &Config,
    policy: &Policy<M::State, M::Action>,
    initial_state_value: Option<StateValue<M::State>>,
) -> StateValue<M::State>
where
    M: MDP,
{
    let states = mdp.get_states();
    let mut state_value = initial_state_value.unwrap_or(StateValue::new(states));
    let mut iteration = 0;
    loop {
        iteration += 1;
        let mut delta: f64 = 0.0;
        for state in states {
            let action = policy.get(state);
            let (next_state, reward) = mdp.transition(state, action);
            let next_state_value = state_value.get(&next_state);
            let new_state_value = reward + config.discount_factor * next_state_value;
            delta = delta.max((new_state_value - state_value.get(state)).abs());
            state_value.insert(state, new_state_value);
        }
        if delta < 1e-5
            || config
                .iterations_before_improvement
                .is_some_and(|n| iteration >= n)
        {
            break;
        }
    }
    state_value
}
```

</CodeAccordion>

### Policy Improvement

Once we have evaluated the _state-value_ function,
we change our policy for a better one according to this _state-value_ function:

$$
\pi'(s) = \text{argmax}_a [r(s, s(s, a)) + \gamma V(s(s, a))]
$$

<CodeAccordion>

```rust
/// # Policy Improvement
///
/// Given an MDP, a discount factor and a state value,
/// this function computes the optimal policy.
pub fn policy_improvement<M>(
    mdp: &M,
    config: &Config,
    state_value: &StateValue<M::State>,
) -> Policy<M::State, M::Action>
where
    M: MDP,
{
    let states = mdp.get_states();
    let actions = mdp.get_actions();
    let mut policy = Policy::new(states, actions);
    for state in states {
        let mut best_action = None;
        let mut best_value = None;
        for action in actions {
            let (next_state, reward) = mdp.transition(state, action);
            let value = reward + config.discount_factor * state_value.get(&next_state);
            if best_value.is_none() || value > best_value.unwrap() {
                best_value = Some(value);
                best_action = Some(action);
            }
        }
        // unwrap is safe because actions is not empty
        policy.insert(state, best_action.unwrap());
    }
    policy
}
```

</CodeAccordion>

### Final Algorithm

![Policy Iteration](/static/posts/gridworld/policy_iteration.svg)

## Value Iteration

In Policy Iteration, we were estimating the _state-value_ function of the policies until reasonable convergence.
In Value Iteration, we instantly greedify the policy between the sweeps for _state-value_ evaluation.
It discards the need for actually computing the policy between each evaluation as it just computes the greedy one on the fly at the moment of choosing an _action_.
In general, a variant of that algorithm is used where we do a few steps of evaluation between two policy improvement steps.
In this case, the code will be the said variant.

![Value Iteration](/static/posts/gridworld/value_iteration.svg)

<CodeAccordion>

```rust
fn policy_value_iteration<M>(mdp: &M, config: &Config) -> StateValue<M::State>
where
    M: MDP,
{
    let states = mdp.get_states();
    let actions = mdp.get_actions();
    let mut state_value = StateValue::new(states);
    let mut policy = Policy::new(states, actions);
    loop {
        state_value = policy_evaluation(mdp, config, &policy, Some(state_value));
        let new_policy = policy_improvement(mdp, config, &state_value);
        if new_policy == policy {
            break;
        }
        policy = new_policy;
    }
    state_value
}
```

</CodeAccordion>

Note that the implementation is shared between Policy Iteration and Value Iteration as they are very similar.
The difference between the two is the `iterations_before_improvement` parameter of the `Config` object.
It is used to do a few steps of evaluation before improving the policy in the case of Value Iteration.
If it is set to None, we evaluate until convergence so the Policy Iteration algorithm is used.

## Temporal Difference Learning

The last two methods we are going to look at are called SARSA and Q-Learning.
They are both examples of Temporal Difference Learning.
The idea is to estimate the _state-value_ function by simulating episodes and updating the _state-value_ function at each step.
The advantage of these mehtods compared to dynamic programming methods like Policy Iteration and Value Iteration
is that they can be used even in the case where the model is not known.

### Exploration vs Exploitation

When simulating episodes in an _environment_, we need to balance between exploration and exploitation.
One very simple way to do that is to take a random _action_ with a certain probability $\varepsilon$ and the greedy _action_ otherwise.
The greedy _action_ represents the exploitation of the current knowledge of the _environment_.
The random _action_ on the other hand represnets the exploration of the _environment_.
Such a policy is called an $\varepsilon$-greedy policy.

### Epsilon Greedy Policy

```rust
/// Returns a random action with probability epsilon
/// or the greedy action with probability 1 - epsilon.
pub fn epsilon_greedy<'a>(&'a self, actions: &'a Sampler<A>, epsilon: f64) -> &A {
    if random::<f64>() < epsilon {
        actions.get_random()
    } else {
        self.greedy()
    }
}
```

## SARSA

With SARSA and $Q$-Learning we are going to be estimating $Q$ instead of $V$ and derive the optimal policy from it.
Another difference with these methods is that we are going to be simulating episodes
through the _gridworld_ in order to estimate the $Q$-function instead of sweeping through the _state_ space $S$.
SARSA stands for State-Action-Reward-State-Action because we are going to look one _action_ forward following the policy to estimate $Q(s,a)$.

## Q-Learning

Q-Learning is basically the same as SARSA, it only differs in its update rule:
instead of choosing a second _action_ following the $\varepsilon$-greedy policy,
we choose the best _action_ available in the next _state_: $\argmax_{a'} a' \rightarrow Q(s', a')$

<CodeAccordion>

```rust
fn sarsa_q_learning<M>(
    mdp: &M,
    config: &Config,
    q_learning: bool,
) -> ActionValue<M::State, M::Action>
where
    M: MDP,
{
    let states = mdp.get_states();
    let actions = mdp.get_actions();
    let mut action_value = ActionValue::new(states, actions);
    for _ in 0..config.num_episodes {
        let mut state = states.get_random().clone();
        let mut action = action_value
            .epsilon_greedy(actions, &state, config.exploration_rate)
            .clone();
        for _ in 0..config.max_num_steps {
            let (next_state, reward) = mdp.transition(&state, &action);
            let next_action = action_value
                .epsilon_greedy(actions, &next_state, config.exploration_rate)
                .clone();
            // update action value
            let current = action_value.get(&state, &action);
            let q_value = if q_learning {
                action_value.get(&next_state, action_value.greedy(&next_state))
            } else {
                action_value.get(&next_state, &next_action)
            };
            let target = reward + config.discount_factor * q_value;
            action_value.insert(
                &state,
                &action,
                current + config.learning_rate * (target - current),
            );
            state = next_state;
            action = next_action;
            if mdp.is_state_terminal(&state) {
                break;
            }
        }
    }
    action_value
}
```

</CodeAccordion>

Remark that we do not completely update $Q(s,a)$ to the new computed _value_.
We make a step in that direction of proportion $\alpha$, this parameter is the
learning rate of our algorithm. It is used to make the algorithm more stable.

## Final Note

If you want to see the actual code used here (TypeScript React), look [here][3].
Also, a lot of simplifications have been done in order to explain more directly
the different concepts. Finally, we only looked at the case of deterministic MDP with known model.
To have a better view of **Reinforcement Learning**, I recommend this [excellent book][4] about it
written by Sutton & Barto, the visual representations from this article are actually directly inspired from that book but remade to fit this website colors.

[1]: https://en.wikipedia.org/wiki/Markov_decision_process
[2]: https://en.wikipedia.org/wiki/Bellman_equation
[3]: https://github.com/devspaceship/devspaceship.com/tree/main/web/src/components/Gridworld
[4]: http://incompleteideas.net/book/RLbook2020.pdf
[5]: https://docs.rs/madepro/0.1.0/madepro/
